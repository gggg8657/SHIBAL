#!/usr/bin/env python3
"""
ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
saved_modelsì˜ í”„ë¦¬íŠ¸ë ˆì¸ë“œ ì›¨ì´íŠ¸ë¡œ ê°„ë‹¨í•œ ì„±ëŠ¥ í™•ì¸
"""

import torch
import numpy as np
from model import Model
import option
from segment_dataset import SegmentDataset
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import os

def quick_test():
    """ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ìˆ˜í–‰"""
    args = option.parse_args()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    print(f"ë””ë°”ì´ìŠ¤: {device}")
    
    # ëª¨ë¸ ë¡œë“œ
    if args.model_arch == 'base':
        model = Model(dropout=args.dropout_rate, attn_dropout=args.attn_dropout_rate)
    elif args.model_arch == 'fast' or args.model_arch == 'tiny':
        model = Model(dropout=args.dropout_rate, attn_dropout=args.attn_dropout_rate, 
                     ff_mult=1, dims=(32,32), depths=(1,1))
    
    # ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ (ê¸°ë³¸ê°’: tiny ëª¨ë¸)
    model_path = 'saved_models/888tiny.pkl'
    if not os.path.exists(model_path):
        print(f"âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
        print("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸:")
        for file in os.listdir('saved_models'):
            if file.endswith('.pkl'):
                print(f"  - saved_models/{file}")
        return
    
    # ëª¨ë¸ íŒŒì¼ëª…ì—ì„œ ì•„í‚¤í…ì²˜ ìë™ ê°ì§€
    if 'base' in model_path or '913' in model_path:
        detected_arch = 'base'
        model = Model(dropout=args.dropout_rate, attn_dropout=args.attn_dropout_rate)
    elif 'tiny' in model_path or '888' in model_path:
        detected_arch = 'tiny'
        model = Model(dropout=args.dropout_rate, attn_dropout=args.attn_dropout_rate, 
                     ff_mult=1, dims=(32,32), depths=(1,1))
    else:
        detected_arch = args.model_arch
    
    print(f"ê°ì§€ëœ ëª¨ë¸ ì•„í‚¤í…ì²˜: {detected_arch}")
    
    try:
        state_dict = torch.load(model_path, map_location=device)
        model.load_state_dict(state_dict, strict=False)
        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path} (strict=False)")
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
    
    model = model.to(device)
    model.eval()
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”
    try:
        test_dataset = SegmentDataset(args, test_mode=True)
        if len(test_dataset) == 0:
            print("âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("custom_data/custom_test.txt íŒŒì¼ì„ ë¨¼ì € ìƒì„±í•˜ì„¸ìš”.")
            return
        
        test_loader = DataLoader(
            test_dataset,
            batch_size=args.batch_size, 
            shuffle=False
        )
        
        print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_dataset)}ê°œ")
        
    except Exception as e:
        print(f"âŒ ë°ì´í„° ë¡œë” ìƒì„± ì‹¤íŒ¨: {e}")
        return
    
    # í…ŒìŠ¤íŠ¸ ìˆ˜í–‰
    predictions = []
    labels = []
    categories = []
    
    print("í…ŒìŠ¤íŠ¸ ì§„í–‰ ì¤‘...")
    with torch.no_grad():
        for batch in test_loader:
            if len(batch) == 4:  # features, label, category, description
                features, label, category, description = batch
            else:
                features, label = batch
                category = ['unknown'] * len(label)
            
            features = features.to(device)
            scores, _ = model(features)
            scores = torch.nn.Sigmoid()(scores).squeeze()
            
            predictions.extend(scores.cpu().numpy())
            labels.extend(label.numpy())
            categories.extend(category)
    
    # ê²°ê³¼ ê³„ì‚°
    predictions = np.array(predictions)
    labels = np.array(labels)
    
    # ROC AUC
    from sklearn.metrics import roc_auc_score, precision_recall_auc_score
    roc_auc = roc_auc_score(labels, predictions)
    
    # ì •í™•ë„
    binary_preds = (predictions > 0.5).astype(int)
    accuracy = np.mean(binary_preds == labels)
    
    print(f"\n=== ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ===")
    print(f"ROC AUC: {roc_auc:.4f}")
    print(f"ì •í™•ë„: {accuracy:.4f}")
    print(f"ì´ ìƒ˜í”Œ: {len(labels)}")
    print(f"ì •ìƒ: {np.sum(labels == 0)}")
    print(f"ë¹„ì •ìƒ: {np.sum(labels == 1)}")
    
    # ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥
    unique_categories = set(categories)
    print(f"\n=== ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥ ===")
    for cat in unique_categories:
        cat_indices = [i for i, c in enumerate(categories) if c == cat]
        if len(cat_indices) > 0:
            cat_preds = predictions[cat_indices]
            cat_labels = labels[cat_indices]
            try:
                cat_auc = roc_auc_score(cat_labels, cat_preds)
                print(f"{cat}: AUC={cat_auc:.4f} (ìƒ˜í”Œ={len(cat_indices)})")
            except:
                print(f"{cat}: AUC ê³„ì‚° ë¶ˆê°€ (ìƒ˜í”Œ={len(cat_indices)})")
    
    # ê°„ë‹¨í•œ ì‹œê°í™”
    try:
        plt.figure(figsize=(10, 4))
        
        # ROC ê³¡ì„ 
        plt.subplot(1, 2, 1)
        from sklearn.metrics import roc_curve
        fpr, tpr, _ = roc_curve(labels, predictions)
        plt.plot(fpr, tpr, label=f'ROC (AUC = {roc_auc:.3f})')
        plt.plot([0, 1], [0, 1], 'k--')
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('ROC Curve')
        plt.legend()
        plt.grid(True)
        
        # ì˜ˆì¸¡ ë¶„í¬
        plt.subplot(1, 2, 2)
        plt.hist(predictions[labels == 0], alpha=0.7, label='Normal', bins=20)
        plt.hist(predictions[labels == 1], alpha=0.7, label='Anomaly', bins=20)
        plt.xlabel('Prediction Score')
        plt.ylabel('Count')
        plt.title('Prediction Distribution')
        plt.legend()
        
        plt.tight_layout()
        plt.savefig('quick_test_results.png', dpi=150, bbox_inches='tight')
        plt.close()
        print("\nğŸ“Š ì‹œê°í™” ê²°ê³¼ê°€ 'quick_test_results.png'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        print(f"ì‹œê°í™” ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    quick_test()
